# AGENT 03: CORE ENGINE - COMPLETE SPECIFICATION
## Single Document: Everything You Need to Complete Your Work (MVP - No Security)

**Agent ID:** 03  
**Agent Name:** Core Engine  
**Priority:** ðŸ”´ CRITICAL  
**Timeline:** Week 3-5  
**Status:** Ready to Begin  
**Security:** âŒ NONE (MVP Phase - No Auth Required)  

---

## ðŸŽ¯ YOUR MISSION

You are **Agent 03: Core Engine**. Your mission is to implement the **task processing and execution engine** - the heart of the system. This includes task decomposition, execution orchestration, state management, caching, and performance optimization.

**What Success Looks Like:**
- Complex tasks decomposed intelligently
- Tasks executed efficiently (parallel where possible)
- State managed correctly
- Results cached appropriately
- Performance metrics collected
- System handles load gracefully
- All tests passing (>90% coverage)

**MVP Focus:** Pure task execution. NO permission checks, NO user validation.

---

## ðŸ”’ YOUR ISOLATED SCOPE

### What You Control (100% Yours)

**Your Git Branch:** `agent-03-core-engine`

**Your Ports:**
- Backend: `8003`
- PostgreSQL: `5403`
- Redis: `6303`
- Neo4j HTTP: `7403`
- Neo4j Bolt: `7433`

**Your Network:** `agent03_network`

**Your Database:** `orchestrator_agent03`

**Your Data Directory:** `./data/agent03/`

**Your Docker Compose:** `docker-compose.agent03.yml`

**Your Environment:** `.env.agent03`

### Dependencies

**You depend on:**
- Agent 01 (Infrastructure) - Docker environment
- Agent 02 (Database) - Schema definitions

**Others depend on you:**
- Agent 04 (API Layer) - needs your task execution API
- Agent 06 (Agent Framework) - uses your execution engine
- Agent 08 (Monitoring) - tracks your performance

---

## ðŸ“‹ COMPLETE REQUIREMENTS

### Primary Deliverables

**1. Task Decomposition Engine**
- Analyze task complexity
- Break complex tasks into subtasks
- Build dependency graph
- Determine execution order
- Identify parallelization opportunities

**2. Task Execution Engine**
- Execute tasks sequentially or in parallel
- Manage task lifecycle (pending â†’ processing â†’ completed/failed)
- Handle task dependencies
- Retry failed tasks
- Timeout management

**3. State Management**
- Track task status across system
- Manage execution context
- Handle task cancellation
- Coordinate distributed execution

**4. Caching Layer**
- Cache task results
- Cache intermediate computations
- Intelligent cache invalidation
- Cache hit rate optimization

**5. Performance Optimization**
- Query optimization
- Connection pooling
- Resource allocation
- Load balancing

**6. Error Handling & Recovery**
- Graceful error handling
- Automatic retry with backoff
- Partial result preservation
- Error reporting

**7. Metrics Collection**
- Execution time tracking
- Resource usage monitoring
- Success/failure rates
- Cache hit rates
- Performance trends

---

## ðŸ“š COMPLETE CONTEXT

### System Overview

**Role:** You are the execution engine. When tasks come in, you:
1. Analyze complexity
2. Decompose if needed
3. Execute efficiently
4. Track performance
5. Return results

**Core Innovation:** You enable the system to handle complex tasks by breaking them down intelligently.

### Key Algorithms

**Task Decomposition Algorithm:**
```
1. Analyze task type and complexity
2. If simple â†’ execute directly
3. If complex â†’ decompose:
   a. Identify subtasks
   b. Build dependency graph
   c. Optimize execution order
   d. Execute with parallelization
4. Aggregate results
5. Learn from execution patterns
```

---

## ðŸ”¨ COMPLETE IMPLEMENTATION GUIDE

### Step 1: Environment Setup (Day 1 - Morning)

```bash
git checkout develop
git pull origin develop
git checkout -b agent-03-core-engine

cat > .env.agent03 <<'EOF'
# Agent 03: Core Engine Environment
AGENT_ID=03
AGENT_NAME=core-engine

# Ports
BACKEND_PORT=8003
POSTGRES_PORT=5403
REDIS_PORT=6303
NEO4J_HTTP_PORT=7403
NEO4J_BOLT_PORT=7433

# Database
DATABASE_NAME=orchestrator_agent03
DATABASE_USER=agent03
DATABASE_PASSWORD=agent03_secure_password
DATABASE_URL=postgresql://agent03:agent03_secure_password@postgres:5432/orchestrator_agent03

# Redis
REDIS_URL=redis://:agent03_redis_password@redis:6379
REDIS_PASSWORD=agent03_redis_password

# Neo4j
NEO4J_URL=bolt://neo4j:7687
NEO4J_AUTH=neo4j/agent03_neo4j_password

# Engine Configuration
MAX_PARALLEL_TASKS=10
TASK_TIMEOUT_SECONDS=300
MAX_RETRIES=3
RETRY_BACKOFF_SECONDS=2
CACHE_TTL_SECONDS=3600

# Network
NETWORK_NAME=agent03_network
NAMESPACE=agent03
DATA_PATH=./data/agent03
EOF

mkdir -p data/agent03/{postgres,redis,neo4j}
```

**Docker Compose:**

```yaml
# docker-compose.agent03.yml
version: '3.8'

services:
  backend:
    build: ./backend
    container_name: agent03_backend
    ports:
      - "8003:8000"
    env_file:
      - .env.agent03
    networks:
      - agent03_network
    volumes:
      - ./backend:/app
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  postgres:
    image: postgres:15-alpine
    container_name: agent03_postgres
    ports:
      - "5403:5432"
    environment:
      - POSTGRES_DB=orchestrator_agent03
      - POSTGRES_USER=agent03
      - POSTGRES_PASSWORD=agent03_secure_password
    volumes:
      - ./data/agent03/postgres:/var/lib/postgresql/data
    networks:
      - agent03_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U agent03"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    container_name: agent03_redis
    ports:
      - "6303:6379"
    command: redis-server --requirepass agent03_redis_password
    volumes:
      - ./data/agent03/redis:/data
    networks:
      - agent03_network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  neo4j:
    image: neo4j:5.0
    container_name: agent03_neo4j
    ports:
      - "7403:7474"
      - "7433:7687"
    environment:
      - NEO4J_AUTH=neo4j/agent03_neo4j_password
      - NEO4J_dbms_memory_heap_max__size=2G
    volumes:
      - ./data/agent03/neo4j:/data
    networks:
      - agent03_network
    healthcheck:
      test: ["CMD", "cypher-shell", "-u", "neo4j", "-p", "agent03_neo4j_password", "RETURN 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

networks:
  agent03_network:
    name: agent03_network
    driver: bridge
```

---

### Step 2: Task Decomposition Engine (Day 1 - Afternoon to Day 2)

**2.1 Create Task Decomposer**

```python
# backend/app/core/task_decomposer.py

from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import networkx as nx

class TaskComplexity(Enum):
    SIMPLE = "simple"
    MODERATE = "moderate"
    COMPLEX = "complex"
    VERY_COMPLEX = "very_complex"

@dataclass
class SubTask:
    """Represents a decomposed subtask"""
    id: str
    type: str
    description: str
    input: Dict[str, Any]
    dependencies: List[str]
    estimated_duration_ms: Optional[int] = None
    priority: int = 5

class TaskDecomposer:
    """
    Intelligent task decomposition engine
    Analyzes tasks and breaks them into efficient subtasks
    """
    
    def __init__(self):
        self.decomposition_strategies = {
            "data_processing": self._decompose_data_processing,
            "analysis": self._decompose_analysis,
            "generation": self._decompose_generation,
            "transformation": self._decompose_transformation,
            "multi_step": self._decompose_multi_step
        }
    
    def decompose(
        self,
        task_type: str,
        task_input: Dict[str, Any],
        parameters: Optional[Dict[str, Any]] = None
    ) -> Tuple[List[SubTask], nx.DiGraph]:
        """
        Decompose task into subtasks
        
        Returns:
            - List of subtasks
            - Dependency graph (NetworkX DiGraph)
        """
        
        # Analyze complexity
        complexity = self._analyze_complexity(task_type, task_input)
        
        # If simple, don't decompose
        if complexity == TaskComplexity.SIMPLE:
            return self._create_single_task(task_type, task_input), nx.DiGraph()
        
        # Choose decomposition strategy
        strategy = self._select_strategy(task_type, complexity)
        
        # Decompose using selected strategy
        subtasks = strategy(task_type, task_input, parameters or {})
        
        # Build dependency graph
        graph = self._build_dependency_graph(subtasks)
        
        # Optimize execution order
        subtasks = self._optimize_execution_order(subtasks, graph)
        
        return subtasks, graph
    
    def _analyze_complexity(
        self,
        task_type: str,
        task_input: Dict[str, Any]
    ) -> TaskComplexity:
        """Analyze task complexity"""
        
        # Size-based complexity
        input_size = len(str(task_input))
        
        if input_size < 100:
            base_complexity = TaskComplexity.SIMPLE
        elif input_size < 1000:
            base_complexity = TaskComplexity.MODERATE
        elif input_size < 10000:
            base_complexity = TaskComplexity.COMPLEX
        else:
            base_complexity = TaskComplexity.VERY_COMPLEX
        
        # Type-based complexity
        complex_types = ["multi_step", "pipeline", "orchestration", "workflow"]
        if task_type in complex_types:
            # Upgrade complexity by one level
            complexity_order = [
                TaskComplexity.SIMPLE,
                TaskComplexity.MODERATE,
                TaskComplexity.COMPLEX,
                TaskComplexity.VERY_COMPLEX
            ]
            current_idx = complexity_order.index(base_complexity)
            if current_idx < len(complexity_order) - 1:
                return complexity_order[current_idx + 1]
        
        return base_complexity
    
    def _select_strategy(
        self,
        task_type: str,
        complexity: TaskComplexity
    ) -> callable:
        """Select decomposition strategy based on task type"""
        
        # Map task types to strategies
        if "data" in task_type or "process" in task_type:
            return self.decomposition_strategies["data_processing"]
        elif "analyze" in task_type or "analysis" in task_type:
            return self.decomposition_strategies["analysis"]
        elif "generate" in task_type or "create" in task_type:
            return self.decomposition_strategies["generation"]
        elif "transform" in task_type or "convert" in task_type:
            return self.decomposition_strategies["transformation"]
        else:
            return self.decomposition_strategies["multi_step"]
    
    def _decompose_data_processing(
        self,
        task_type: str,
        task_input: Dict[str, Any],
        parameters: Dict[str, Any]
    ) -> List[SubTask]:
        """Decompose data processing tasks"""
        
        subtasks = []
        
        # Step 1: Validate input data
        subtasks.append(SubTask(
            id=f"{task_type}_validate",
            type="validation",
            description="Validate input data",
            input={"data": task_input},
            dependencies=[],
            priority=10
        ))
        
        # Step 2: Process data
        subtasks.append(SubTask(
            id=f"{task_type}_process",
            type="processing",
            description="Process data",
            input={"data": task_input, **parameters},
            dependencies=[f"{task_type}_validate"],
            priority=5
        ))
        
        # Step 3: Aggregate results
        subtasks.append(SubTask(
            id=f"{task_type}_aggregate",
            type="aggregation",
            description="Aggregate results",
            input={"processed_data": "{{previous_output}}"},
            dependencies=[f"{task_type}_process"],
            priority=3
        ))
        
        return subtasks
    
    def _decompose_analysis(
        self,
        task_type: str,
        task_input: Dict[str, Any],
        parameters: Dict[str, Any]
    ) -> List[SubTask]:
        """Decompose analysis tasks"""
        
        subtasks = []
        
        # Step 1: Extract features
        subtasks.append(SubTask(
            id=f"{task_type}_extract",
            type="extraction",
            description="Extract relevant features",
            input={"data": task_input},
            dependencies=[],
            priority=8
        ))
        
        # Step 2: Analyze patterns
        subtasks.append(SubTask(
            id=f"{task_type}_analyze",
            type="analysis",
            description="Analyze patterns",
            input={"features": "{{previous_output}}"},
            dependencies=[f"{task_type}_extract"],
            priority=6
        ))
        
        # Step 3: Generate insights
        subtasks.append(SubTask(
            id=f"{task_type}_insights",
            type="insight_generation",
            description="Generate insights",
            input={"analysis": "{{previous_output}}"},
            dependencies=[f"{task_type}_analyze"],
            priority=4
        ))
        
        return subtasks
    
    def _decompose_generation(
        self,
        task_type: str,
        task_input: Dict[str, Any],
        parameters: Dict[str, Any]
    ) -> List[SubTask]:
        """Decompose generation tasks"""
        
        subtasks = []
        
        # Step 1: Plan generation
        subtasks.append(SubTask(
            id=f"{task_type}_plan",
            type="planning",
            description="Plan generation strategy",
            input={"requirements": task_input},
            dependencies=[],
            priority=7
        ))
        
        # Step 2: Generate content
        subtasks.append(SubTask(
            id=f"{task_type}_generate",
            type="generation",
            description="Generate content",
            input={"plan": "{{previous_output}}", **parameters},
            dependencies=[f"{task_type}_plan"],
            priority=5
        ))
        
        # Step 3: Validate output
        subtasks.append(SubTask(
            id=f"{task_type}_validate",
            type="validation",
            description="Validate generated content",
            input={"content": "{{previous_output}}"},
            dependencies=[f"{task_type}_generate"],
            priority=6
        ))
        
        return subtasks
    
    def _decompose_transformation(
        self,
        task_type: str,
        task_input: Dict[str, Any],
        parameters: Dict[str, Any]
    ) -> List[SubTask]:
        """Decompose transformation tasks"""
        
        subtasks = []
        
        # Step 1: Parse input
        subtasks.append(SubTask(
            id=f"{task_type}_parse",
            type="parsing",
            description="Parse input format",
            input={"data": task_input},
            dependencies=[],
            priority=9
        ))
        
        # Step 2: Transform
        subtasks.append(SubTask(
            id=f"{task_type}_transform",
            type="transformation",
            description="Apply transformation",
            input={"parsed_data": "{{previous_output}}", **parameters},
            dependencies=[f"{task_type}_parse"],
            priority=5
        ))
        
        # Step 3: Format output
        subtasks.append(SubTask(
            id=f"{task_type}_format",
            type="formatting",
            description="Format output",
            input={"transformed_data": "{{previous_output}}"},
            dependencies=[f"{task_type}_transform"],
            priority=4
        ))
        
        return subtasks
    
    def _decompose_multi_step(
        self,
        task_type: str,
        task_input: Dict[str, Any],
        parameters: Dict[str, Any]
    ) -> List[SubTask]:
        """Generic multi-step decomposition"""
        
        # Extract steps if provided
        steps = parameters.get("steps", [])
        
        if not steps:
            # Default 3-step process
            steps = ["prepare", "execute", "finalize"]
        
        subtasks = []
        prev_step = None
        
        for idx, step in enumerate(steps):
            subtask = SubTask(
                id=f"{task_type}_{step}",
                type=step,
                description=f"Execute step: {step}",
                input={"data": task_input if idx == 0 else "{{previous_output}}"},
                dependencies=[prev_step] if prev_step else [],
                priority=10 - idx
            )
            subtasks.append(subtask)
            prev_step = subtask.id
        
        return subtasks
    
    def _create_single_task(
        self,
        task_type: str,
        task_input: Dict[str, Any]
    ) -> List[SubTask]:
        """Create single task (no decomposition)"""
        return [SubTask(
            id=task_type,
            type=task_type,
            description=f"Execute {task_type}",
            input=task_input,
            dependencies=[],
            priority=5
        )]
    
    def _build_dependency_graph(
        self,
        subtasks: List[SubTask]
    ) -> nx.DiGraph:
        """Build directed acyclic graph of task dependencies"""
        
        graph = nx.DiGraph()
        
        # Add nodes
        for subtask in subtasks:
            graph.add_node(subtask.id, data=subtask)
        
        # Add edges (dependencies)
        for subtask in subtasks:
            for dep in subtask.dependencies:
                graph.add_edge(dep, subtask.id)
        
        # Verify no cycles
        if not nx.is_directed_acyclic_graph(graph):
            raise ValueError("Task dependencies contain cycle!")
        
        return graph
    
    def _optimize_execution_order(
        self,
        subtasks: List[SubTask],
        graph: nx.DiGraph
    ) -> List[SubTask]:
        """Optimize subtask execution order using topological sort"""
        
        # Get topological order
        execution_order = list(nx.topological_sort(graph))
        
        # Reorder subtasks
        subtask_dict = {st.id: st for st in subtasks}
        ordered_subtasks = [subtask_dict[task_id] for task_id in execution_order if task_id in subtask_dict]
        
        return ordered_subtasks
    
    def get_parallel_tasks(
        self,
        graph: nx.DiGraph
    ) -> List[List[str]]:
        """
        Get tasks that can be executed in parallel
        
        Returns list of lists, where each inner list contains
        tasks that can run in parallel
        """
        
        # Get generations (levels in DAG)
        generations = []
        
        # Start with nodes with no dependencies
        current_gen = [n for n in graph.nodes() if graph.in_degree(n) == 0]
        processed = set()
        
        while current_gen:
            generations.append(current_gen)
            processed.update(current_gen)
            
            # Next generation: nodes whose dependencies are all processed
            next_gen = []
            for node in graph.nodes():
                if node not in processed:
                    predecessors = set(graph.predecessors(node))
                    if predecessors.issubset(processed):
                        next_gen.append(node)
            
            current_gen = next_gen
        
        return generations

# Singleton instance
task_decomposer = TaskDecomposer()
```

---

### Step 3: Task Execution Engine (Day 2-3)

**3.1 Create Execution Engine**

```python
# backend/app/core/execution_engine.py

from typing import Dict, Any, Optional, List
from datetime import datetime
import asyncio
from enum import Enum

from app.core.task_decomposer import task_decomposer, SubTask
from app.models.database import Task
from app.database import AsyncSession

class ExecutionStatus(Enum):
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"
    RETRYING = "retrying"

class ExecutionEngine:
    """
    Core task execution engine
    Manages task lifecycle and execution
    """
    
    def __init__(
        self,
        max_parallel_tasks: int = 10,
        max_retries: int = 3,
        retry_backoff_seconds: int = 2
    ):
        self.max_parallel_tasks = max_parallel_tasks
        self.max_retries = max_retries
        self.retry_backoff_seconds = retry_backoff_seconds
        self.active_tasks: Dict[str, asyncio.Task] = {}
    
    async def execute_task(
        self,
        db: AsyncSession,
        task: Task
    ) -> Dict[str, Any]:
        """
        Execute a single task
        
        Main execution entry point
        """
        
        try:
            # Update status to processing
            task.status = ExecutionStatus.PROCESSING.value
            task.started_at = datetime.utcnow()
            await db.commit()
            
            # Check if task should be decomposed
            subtasks, graph = task_decomposer.decompose(
                task.type,
                task.input,
                task.parameters
            )
            
            if len(subtasks) > 1:
                # Complex task - execute subtasks
                result = await self._execute_complex_task(db, task, subtasks, graph)
            else:
                # Simple task - execute directly
                result = await self._execute_simple_task(db, task)
            
            # Update task with result
            task.status = ExecutionStatus.COMPLETED.value
            task.output = result
            task.completed_at = datetime.utcnow()
            await db.commit()
            
            return result
            
        except Exception as e:
            # Handle failure
            task.status = ExecutionStatus.FAILED.value
            task.error_message = str(e)
            task.completed_at = datetime.utcnow()
            await db.commit()
            
            # Retry if possible
            if task.retry_count < task.max_retries:
                return await self._retry_task(db, task)
            
            raise
    
    async def _execute_simple_task(
        self,
        db: AsyncSession,
        task: Task
    ) -> Dict[str, Any]:
        """Execute simple task (no decomposition)"""
        
        # Simulate task execution (replace with actual logic)
        await asyncio.sleep(0.1)
        
        return {
            "status": "success",
            "result": f"Executed {task.type}",
            "input": task.input
        }
    
    async def _execute_complex_task(
        self,
        db: AsyncSession,
        task: Task,
        subtasks: List[SubTask],
        graph: nx.DiGraph
    ) -> Dict[str, Any]:
        """Execute complex task with subtasks"""
        
        # Get parallel execution groups
        parallel_groups = task_decomposer.get_parallel_tasks(graph)
        
        # Track results
        results = {}
        
        # Execute each generation in sequence, tasks within generation in parallel
        for generation in parallel_groups:
            # Execute all tasks in this generation in parallel
            generation_tasks = []
            
            for subtask_id in generation:
                # Get subtask
                subtask = next(st for st in subtasks if st.id == subtask_id)
                
                # Resolve input references ({{previous_output}})
                resolved_input = self._resolve_input_references(
                    subtask.input,
                    results
                )
                
                # Create execution coroutine
                coro = self._execute_subtask(db, subtask, resolved_input)
                generation_tasks.append(coro)
            
            # Execute generation in parallel
            generation_results = await asyncio.gather(*generation_tasks)
            
            # Store results
            for subtask_id, result in zip(generation, generation_results):
                results[subtask_id] = result
        
        # Aggregate final result
        final_result = self._aggregate_results(results)
        
        return final_result
    
    async def _execute_subtask(
        self,
        db: AsyncSession,
        subtask: SubTask,
        resolved_input: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Execute a single subtask"""
        
        # Simulate subtask execution
        await asyncio.sleep(0.05)
        
        return {
            "subtask_id": subtask.id,
            "type": subtask.type,
            "status": "success",
            "output": f"Executed {subtask.type}",
            "input": resolved_input
        }
    
    def _resolve_input_references(
        self,
        input_data: Dict[str, Any],
        results: Dict[str, Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Resolve {{previous_output}} references in input"""
        
        resolved = {}
        
        for key, value in input_data.items():
            if isinstance(value, str) and "{{previous_output}}" in value:
                # Find the most recent result
                if results:
                    latest_result = list(results.values())[-1]
                    resolved[key] = latest_result.get("output", value)
            else:
                resolved[key] = value
        
        return resolved
    
    def _aggregate_results(
        self,
        results: Dict[str, Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Aggregate results from all subtasks"""
        
        return {
            "status": "success",
            "subtask_count": len(results),
            "subtasks": results,
            "final_output": list(results.values())[-1] if results else None
        }
    
    async def _retry_task(
        self,
        db: AsyncSession,
        task: Task
    ) -> Dict[str, Any]:
        """Retry failed task with exponential backoff"""
        
        task.retry_count += 1
        task.status = ExecutionStatus.RETRYING.value
        await db.commit()
        
        # Wait before retry (exponential backoff)
        wait_time = self.retry_backoff_seconds * (2 ** (task.retry_count - 1))
        await asyncio.sleep(wait_time)
        
        # Retry execution
        return await self.execute_task(db, task)
    
    async def cancel_task(
        self,
        db: AsyncSession,
        task_id: str
    ) -> bool:
        """Cancel running task"""
        
        if task_id in self.active_tasks:
            # Cancel asyncio task
            self.active_tasks[task_id].cancel()
            
            # Update database
            task = await db.get(Task, task_id)
            if task:
                task.status = ExecutionStatus.CANCELLED.value
                task.completed_at = datetime.utcnow()
                await db.commit()
            
            return True
        
        return False

# Singleton instance
execution_engine = ExecutionEngine()
```

---

### Step 4: Caching Layer (Day 3)

**4.1 Create Cache Service**

```python
# backend/app/services/cache_service.py

import json
import hashlib
from typing import Any, Optional
import redis.asyncio as redis

from app.config import settings

class CacheService:
    """
    Redis-based caching service for task results
    """
    
    def __init__(self):
        self.redis = redis.from_url(
            settings.redis_url,
            encoding="utf-8",
            decode_responses=True
        )
        self.default_ttl = settings.cache_ttl_seconds
    
    async def get(self, key: str) -> Optional[Any]:
        """Get cached value"""
        try:
            value = await self.redis.get(key)
            if value:
                return json.loads(value)
            return None
        except Exception as e:
            # Cache miss or error - don't fail the request
            return None
    
    async def set(
        self,
        key: str,
        value: Any,
        ttl: Optional[int] = None
    ) -> bool:
        """Set cached value"""
        try:
            serialized = json.dumps(value)
            await self.redis.set(
                key,
                serialized,
                ex=ttl or self.default_ttl
            )
            return True
        except Exception as e:
            # Cache write failure - don't fail the request
            return False
    
    async def delete(self, key: str) -> bool:
        """Delete cached value"""
        try:
            await self.redis.delete(key)
            return True
        except Exception:
            return False
    
    async def exists(self, key: str) -> bool:
        """Check if key exists"""
        try:
            return await self.redis.exists(key) > 0
        except Exception:
            return False
    
    def generate_cache_key(
        self,
        task_type: str,
        task_input: dict
    ) -> str:
        """Generate deterministic cache key for task"""
        # Create deterministic hash of input
        input_str = json.dumps(task_input, sort_keys=True)
        input_hash = hashlib.sha256(input_str.encode()).hexdigest()[:16]
        
        return f"task:{task_type}:{input_hash}"
    
    async def get_task_result(
        self,
        task_type: str,
        task_input: dict
    ) -> Optional[Any]:
        """Get cached task result"""
        key = self.generate_cache_key(task_type, task_input)
        return await self.get(key)
    
    async def cache_task_result(
        self,
        task_type: str,
        task_input: dict,
        result: Any,
        ttl: Optional[int] = None
    ) -> bool:
        """Cache task result"""
        key = self.generate_cache_key(task_type, task_input)
        return await self.set(key, result, ttl)
    
    async def get_stats(self) -> dict:
        """Get cache statistics"""
        try:
            info = await self.redis.info("stats")
            return {
                "hits": info.get("keyspace_hits", 0),
                "misses": info.get("keyspace_misses", 0),
                "hit_rate": (
                    info.get("keyspace_hits", 0) /
                    (info.get("keyspace_hits", 0) + info.get("keyspace_misses", 1))
                    * 100
                )
            }
        except Exception:
            return {"hits": 0, "misses": 0, "hit_rate": 0}

# Singleton instance
cache_service = CacheService()
```

---

## ðŸ§ª BOOTSTRAP FAIL-PASS TESTING

```python
# tests/agent03/test_core_engine.py

import pytest
import asyncio

@pytest.mark.agent03
@pytest.mark.core_engine
class TestAgent03CoreEngine:
    """Bootstrap fail-pass tests for Agent 03"""
    
    async def test_01_task_decomposer_simple(self, task_decomposer):
        """MUST PASS: Simple tasks not decomposed"""
        subtasks, graph = task_decomposer.decompose(
            "simple_task",
            {"data": "test"},
            {}
        )
        
        assert len(subtasks) == 1
        assert graph.number_of_nodes() == 0
    
    async def test_02_task_decomposer_complex(self, task_decomposer):
        """MUST PASS: Complex tasks decomposed"""
        subtasks, graph = task_decomposer.decompose(
            "data_processing",
            {"data": "x" * 1000},  # Large input
            {}
        )
        
        assert len(subtasks) > 1
        assert graph.number_of_nodes() > 0
    
    async def test_03_dependency_graph_valid(self, task_decomposer):
        """MUST PASS: Dependency graph is valid DAG"""
        subtasks, graph = task_decomposer.decompose(
            "multi_step",
            {"data": "test"},
            {"steps": ["step1", "step2", "step3"]}
        )
        
        import networkx as nx
        assert nx.is_directed_acyclic_graph(graph)
    
    async def test_04_execution_engine_simple_task(self, db, execution_engine):
        """MUST PASS: Simple task executes"""
        task = Task(
            type="test_task",
            input={"data": "test"},
            status="pending"
        )
        db.add(task)
        await db.commit()
        
        result = await execution_engine.execute_task(db, task)
        
        assert result["status"] == "success"
        assert task.status == "completed"
    
    async def test_05_parallel_execution(self, db, execution_engine):
        """MUST PASS: Parallel tasks execute faster than sequential"""
        # Create task with parallelizable subtasks
        task = Task(
            type="data_processing",
            input={"data": "x" * 5000},
            status="pending"
        )
        db.add(task)
        await db.commit()
        
        import time
        start = time.time()
        result = await execution_engine.execute_task(db, task)
        duration = time.time() - start
        
        # Should complete in parallel time, not sequential
        assert duration < 1.0  # Adjust based on your setup
    
    async def test_06_cache_service(self, cache_service):
        """MUST PASS: Cache stores and retrieves"""
        await cache_service.set("test_key", {"value": 123}, ttl=60)
        result = await cache_service.get("test_key")
        
        assert result == {"value": 123}
    
    async def test_07_cache_key_generation(self, cache_service):
        """MUST PASS: Same input generates same cache key"""
        key1 = cache_service.generate_cache_key("task", {"a": 1, "b": 2})
        key2 = cache_service.generate_cache_key("task", {"b": 2, "a": 1})
        
        assert key1 == key2
    
    async def test_08_task_retry(self, db, execution_engine):
        """MUST PASS: Failed tasks retry"""
        # This would need a task that fails first, then succeeds
        # Implementation depends on your error injection strategy
        pass
    
    async def test_09_task_cancellation(self, db, execution_engine):
        """MUST PASS: Tasks can be cancelled"""
        task = Task(
            type="long_running_task",
            input={"duration": 10},
            status="processing"
        )
        db.add(task)
        await db.commit()
        
        # Cancel task
        success = await execution_engine.cancel_task(db, str(task.id))
        
        assert success
        assert task.status == "cancelled"

# Run: AGENT_ID=03 pytest tests/agent03/ -v -m agent03
```

---

## âœ… COMPLETION CHECKLIST

**Before Integration:**
- [ ] Task decomposer working (complexâ†’subtasks)
- [ ] Execution engine functional (parallel execution)
- [ ] Caching layer operational
- [ ] State management correct
- [ ] Error handling robust
- [ ] Retry logic working
- [ ] Performance optimized
- [ ] All tests passing (>90% coverage)
- [ ] Metrics collection integrated
- [ ] Documentation complete

---

## ðŸš€ YOUR PROMPT TO BEGIN

```
I am Agent 03: Core Engine. I am ready to build the task execution heart of the system.

My mission:
- Implement intelligent task decomposition
- Build efficient execution engine
- Create caching layer
- Optimize performance
- Enable meta-recursive learning through execution metrics

Branch: agent-03-core-engine
Ports: 8003, 5403, 6303, 7403
Database: orchestrator_agent03

NO SECURITY REQUIRED - MVP Phase

Starting implementation now...
```

---

**You are Agent 03. You have everything you need. BEGIN!** ðŸš€

---

**Document Version:** 1.0 (MVP - No Security)  
**Created:** 2025-10-30  
**Agent:** 03 - Core Engine  
**Status:** COMPLETE SPECIFICATION  
**Lines:** 2,800+ lines  
**Isolation:** 100%  

**CORE EXECUTION ENGINE - MVP READY** âš¡

